{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Sentiment Assignment\n",
    "\n",
    "This notebook holds the Sentiment Assignment for Module 6 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In a previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we apply sentiment analysis to those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"/Users/home/Documents/GitHub/ADS509_2.1/M1 Results/\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\"\n",
    "\n",
    "positive_words_file = \"positive-words.txt\"\n",
    "negative_words_file = \"negative-words.txt\"\n",
    "tidy_text_file = \"tidytext_sentiments.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A Pandas data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d70801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded lyrics for 2 artists:\n",
      "  robyn: 104 songs\n",
      "  cher: 316 songs\n"
     ]
    }
   ],
   "source": [
    "# Read in the lyrics data\n",
    "lyrics_data = {}\n",
    "lyrics_path = Path(data_location) / lyrics_folder\n",
    "\n",
    "# Get all artist directories\n",
    "for artist_dir in lyrics_path.iterdir():\n",
    "    if artist_dir.is_dir():\n",
    "        artist_name = artist_dir.name\n",
    "        lyrics_data[artist_name] = {}\n",
    "        \n",
    "        # Read all song files for this artist\n",
    "        for song_file in artist_dir.glob('*.txt'):\n",
    "            song_name = song_file.stem.replace(f'{artist_name}_', '')\n",
    "            try:\n",
    "                with open(song_file, 'r', encoding='utf-8') as f:\n",
    "                    lyrics_data[artist_name][song_name] = f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                # Try with different encoding if utf-8 fails\n",
    "                with open(song_file, 'r', encoding='latin-1') as f:\n",
    "                    lyrics_data[artist_name][song_name] = f.read()\n",
    "\n",
    "print(f\"Loaded lyrics for {len(lyrics_data)} artists:\")\n",
    "for artist, songs in lyrics_data.items():\n",
    "    print(f\"  {artist}: {len(songs)} songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debcac5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Twitter data for cher...\n",
      "  Processed 10000 lines...\n",
      "  Processed 20000 lines...\n",
      "  Processed 30000 lines...\n",
      "  Processed 40000 lines...\n",
      "  Processed 50000 lines...\n",
      "  Processed 60000 lines...\n",
      "  Processed 70000 lines...\n",
      "  Processed 80000 lines...\n",
      "  Processed 90000 lines...\n",
      "  Processed 100000 lines...\n",
      "  Processed 110000 lines...\n",
      "  Processed 120000 lines...\n",
      "  Processed 130000 lines...\n",
      "  Processed 140000 lines...\n",
      "  Processed 150000 lines...\n",
      "  Processed 160000 lines...\n",
      "  Processed 170000 lines...\n",
      "  Processed 180000 lines...\n",
      "  Processed 190000 lines...\n",
      "  Processed 200000 lines...\n",
      "  Processed 210000 lines...\n",
      "  Processed 220000 lines...\n",
      "  Processed 230000 lines...\n",
      "  Processed 240000 lines...\n",
      "  Processed 250000 lines...\n",
      "  Processed 260000 lines...\n",
      "  Processed 270000 lines...\n",
      "  Processed 280000 lines...\n",
      "  Processed 290000 lines...\n",
      "  Processed 300000 lines...\n",
      "  Processed 310000 lines...\n",
      "  Processed 320000 lines...\n",
      "  Processed 330000 lines...\n",
      "  Processed 340000 lines...\n",
      "  Processed 350000 lines...\n",
      "  Processed 360000 lines...\n",
      "  Processed 370000 lines...\n",
      "  Processed 380000 lines...\n",
      "  Processed 390000 lines...\n",
      "  Processed 400000 lines...\n",
      "  Processed 410000 lines...\n",
      "  Processed 420000 lines...\n",
      "  Processed 430000 lines...\n",
      "  Processed 440000 lines...\n",
      "  Processed 450000 lines...\n",
      "  Processed 460000 lines...\n",
      "  Processed 470000 lines...\n",
      "  Processed 480000 lines...\n",
      "  Processed 490000 lines...\n",
      "  Processed 500000 lines...\n",
      "  Processed 510000 lines...\n",
      "  Processed 520000 lines...\n",
      "  Processed 530000 lines...\n",
      "  Processed 540000 lines...\n",
      "  Processed 550000 lines...\n",
      "  Processed 560000 lines...\n",
      "  Processed 570000 lines...\n",
      "  Processed 580000 lines...\n",
      "  Processed 590000 lines...\n",
      "  Processed 600000 lines...\n",
      "  Processed 610000 lines...\n",
      "  Processed 620000 lines...\n",
      "  Processed 630000 lines...\n",
      "  Processed 640000 lines...\n",
      "  Processed 650000 lines...\n",
      "  Processed 660000 lines...\n",
      "  Processed 670000 lines...\n",
      "  Processed 680000 lines...\n",
      "  Processed 690000 lines...\n",
      "  Processed 700000 lines...\n",
      "  Processed 710000 lines...\n",
      "  Processed 720000 lines...\n",
      "  Processed 730000 lines...\n",
      "  Processed 740000 lines...\n",
      "  Processed 750000 lines...\n",
      "  Processed 760000 lines...\n",
      "  Processed 770000 lines...\n",
      "  Processed 780000 lines...\n",
      "  Processed 790000 lines...\n",
      "  Processed 800000 lines...\n",
      "  Processed 810000 lines...\n",
      "  Processed 820000 lines...\n",
      "  Processed 830000 lines...\n",
      "  Processed 840000 lines...\n",
      "  Processed 850000 lines...\n",
      "  Processed 860000 lines...\n",
      "  Processed 870000 lines...\n",
      "  Processed 880000 lines...\n",
      "  Processed 890000 lines...\n",
      "  Processed 900000 lines...\n",
      "  Processed 910000 lines...\n",
      "  Processed 920000 lines...\n",
      "  Processed 930000 lines...\n",
      "  Processed 940000 lines...\n",
      "  Processed 950000 lines...\n",
      "  Processed 960000 lines...\n",
      "  Processed 970000 lines...\n",
      "  Processed 980000 lines...\n",
      "  Processed 990000 lines...\n",
      "  Processed 1000000 lines...\n",
      "  Processed 1010000 lines...\n",
      "  Processed 1020000 lines...\n",
      "  Processed 1030000 lines...\n",
      "  Processed 1040000 lines...\n",
      "  Processed 1050000 lines...\n",
      "  Processed 1060000 lines...\n",
      "  Processed 1070000 lines...\n",
      "  Processed 1080000 lines...\n",
      "  Processed 1090000 lines...\n",
      "  Processed 1100000 lines...\n",
      "  Processed 1110000 lines...\n",
      "  Processed 1120000 lines...\n",
      "  Processed 1130000 lines...\n",
      "  Processed 1140000 lines...\n",
      "  Processed 1150000 lines...\n",
      "  Processed 1160000 lines...\n",
      "  Processed 1170000 lines...\n",
      "  Processed 1180000 lines...\n",
      "  Processed 1190000 lines...\n",
      "  Processed 1200000 lines...\n",
      "  Processed 1210000 lines...\n",
      "  Processed 1220000 lines...\n",
      "  Processed 1230000 lines...\n",
      "  Processed 1240000 lines...\n",
      "  Processed 1250000 lines...\n",
      "  Processed 1260000 lines...\n",
      "  Processed 1270000 lines...\n",
      "  Processed 1280000 lines...\n",
      "  Processed 1290000 lines...\n",
      "  Processed 1300000 lines...\n",
      "  Processed 1310000 lines...\n",
      "  Processed 1320000 lines...\n",
      "  Processed 1330000 lines...\n",
      "  Processed 1340000 lines...\n",
      "  Processed 1350000 lines...\n",
      "  Processed 1360000 lines...\n",
      "  Processed 1370000 lines...\n",
      "  Processed 1380000 lines...\n",
      "  Processed 1390000 lines...\n",
      "  Processed 1400000 lines...\n",
      "  Processed 1410000 lines...\n",
      "  Processed 1420000 lines...\n",
      "  Processed 1430000 lines...\n",
      "  Processed 1440000 lines...\n",
      "  Processed 1450000 lines...\n",
      "  Processed 1460000 lines...\n",
      "  Processed 1470000 lines...\n",
      "  Processed 1480000 lines...\n",
      "  Processed 1490000 lines...\n",
      "  Processed 1500000 lines...\n",
      "  Processed 1510000 lines...\n",
      "  Processed 1520000 lines...\n",
      "  Processed 1530000 lines...\n",
      "  Processed 1540000 lines...\n",
      "  Processed 1550000 lines...\n",
      "  Processed 1560000 lines...\n",
      "  Processed 1570000 lines...\n",
      "  Processed 1580000 lines...\n",
      "  Processed 1590000 lines...\n",
      "  Processed 1600000 lines...\n",
      "  Processed 1610000 lines...\n",
      "  Processed 1620000 lines...\n",
      "  Processed 1630000 lines...\n",
      "  Processed 1640000 lines...\n",
      "  Processed 1650000 lines...\n",
      "  Processed 1660000 lines...\n",
      "  Processed 1670000 lines...\n",
      "  Processed 1680000 lines...\n",
      "  Processed 1690000 lines...\n",
      "  Processed 1700000 lines...\n",
      "  Processed 1710000 lines...\n",
      "  Processed 1720000 lines...\n",
      "  Processed 1730000 lines...\n",
      "  Processed 1740000 lines...\n",
      "  Processed 1750000 lines...\n",
      "  Processed 1760000 lines...\n",
      "  Processed 1770000 lines...\n",
      "  Processed 1780000 lines...\n",
      "  Processed 1790000 lines...\n",
      "  Processed 1800000 lines...\n",
      "  Processed 1810000 lines...\n",
      "  Processed 1820000 lines...\n",
      "  Processed 1830000 lines...\n",
      "  Processed 1840000 lines...\n",
      "  Processed 1850000 lines...\n",
      "  Processed 1860000 lines...\n",
      "  Processed 1870000 lines...\n",
      "  Processed 1880000 lines...\n",
      "  Processed 1890000 lines...\n",
      "  Processed 1900000 lines...\n",
      "  Processed 1910000 lines...\n",
      "  Processed 1920000 lines...\n",
      "  Processed 1930000 lines...\n",
      "  Processed 1940000 lines...\n",
      "  Processed 1950000 lines...\n",
      "  Processed 1960000 lines...\n",
      "  Processed 1970000 lines...\n",
      "  Processed 1980000 lines...\n",
      "  Processed 1990000 lines...\n",
      "  Processed 2000000 lines...\n",
      "  Processed 2010000 lines...\n",
      "  Processed 2020000 lines...\n",
      "  Processed 2030000 lines...\n",
      "  Processed 2040000 lines...\n",
      "  Processed 2050000 lines...\n",
      "  Processed 2060000 lines...\n",
      "  Processed 2070000 lines...\n",
      "  Processed 2080000 lines...\n",
      "  Processed 2090000 lines...\n",
      "  Processed 2100000 lines...\n",
      "  Processed 2110000 lines...\n",
      "  Processed 2120000 lines...\n",
      "  Processed 2130000 lines...\n",
      "  Processed 2140000 lines...\n",
      "  Processed 2150000 lines...\n",
      "  Processed 2160000 lines...\n",
      "  Processed 2170000 lines...\n",
      "  Processed 2180000 lines...\n",
      "  Processed 2190000 lines...\n",
      "  Processed 2200000 lines...\n",
      "  Processed 2210000 lines...\n",
      "  Processed 2220000 lines...\n",
      "  Processed 2230000 lines...\n",
      "  Processed 2240000 lines...\n",
      "  Processed 2250000 lines...\n",
      "  Processed 2260000 lines...\n",
      "  Processed 2270000 lines...\n",
      "  Processed 2280000 lines...\n",
      "  Processed 2290000 lines...\n",
      "  Processed 2300000 lines...\n",
      "  Processed 2310000 lines...\n",
      "  Processed 2320000 lines...\n",
      "  Processed 2330000 lines...\n",
      "  Processed 2340000 lines...\n",
      "  Processed 2350000 lines...\n",
      "  Processed 2360000 lines...\n",
      "  Processed 2370000 lines...\n",
      "  Processed 2380000 lines...\n",
      "  Processed 2390000 lines...\n",
      "  Processed 2400000 lines...\n",
      "  Processed 2410000 lines...\n",
      "  Processed 2420000 lines...\n",
      "  Processed 2430000 lines...\n",
      "  Processed 2440000 lines...\n",
      "  Processed 2450000 lines...\n",
      "  Processed 2460000 lines...\n",
      "  Processed 2470000 lines...\n",
      "  Processed 2480000 lines...\n",
      "  Processed 2490000 lines...\n",
      "  Processed 2500000 lines...\n",
      "  Processed 2510000 lines...\n",
      "  Processed 2520000 lines...\n",
      "  Processed 2530000 lines...\n",
      "  Processed 2540000 lines...\n",
      "  Processed 2550000 lines...\n",
      "  Processed 2560000 lines...\n",
      "  Processed 2570000 lines...\n",
      "  Processed 2580000 lines...\n",
      "  Processed 2590000 lines...\n",
      "  Processed 2600000 lines...\n",
      "  Processed 2610000 lines...\n",
      "  Processed 2620000 lines...\n",
      "  Processed 2630000 lines...\n",
      "  Processed 2640000 lines...\n",
      "  Processed 2650000 lines...\n",
      "  Processed 2660000 lines...\n",
      "  Processed 2670000 lines...\n",
      "  Processed 2680000 lines...\n",
      "  Processed 2690000 lines...\n",
      "  Processed 2700000 lines...\n",
      "  Processed 2710000 lines...\n",
      "  Processed 2720000 lines...\n",
      "  Processed 2730000 lines...\n",
      "  Processed 2740000 lines...\n",
      "  Processed 2750000 lines...\n",
      "  Processed 2760000 lines...\n",
      "  Processed 2770000 lines...\n",
      "  Processed 2780000 lines...\n",
      "  Processed 2790000 lines...\n",
      "  Processed 2800000 lines...\n",
      "  Processed 2810000 lines...\n",
      "  Processed 2820000 lines...\n",
      "  Processed 2830000 lines...\n",
      "  Processed 2840000 lines...\n",
      "  Processed 2850000 lines...\n",
      "  Processed 2860000 lines...\n",
      "  Processed 2870000 lines...\n",
      "  Processed 2880000 lines...\n",
      "  Processed 2890000 lines...\n",
      "  Processed 2900000 lines...\n",
      "  Processed 2910000 lines...\n",
      "  Processed 2920000 lines...\n",
      "  Processed 2930000 lines...\n",
      "  Processed 2940000 lines...\n",
      "  Processed 2950000 lines...\n",
      "  Processed 2960000 lines...\n",
      "  Processed 2970000 lines...\n",
      "  Processed 2980000 lines...\n",
      "  Processed 2990000 lines...\n",
      "  Processed 3000000 lines...\n",
      "  Processed 3010000 lines...\n",
      "  Processed 3020000 lines...\n",
      "  Processed 3030000 lines...\n",
      "  Processed 3040000 lines...\n",
      "  Processed 3050000 lines...\n",
      "  Processed 3060000 lines...\n",
      "  Processed 3070000 lines...\n",
      "  Processed 3080000 lines...\n",
      "  Processed 3090000 lines...\n",
      "  Processed 3100000 lines...\n",
      "  Processed 3110000 lines...\n",
      "  Processed 3120000 lines...\n",
      "  Processed 3130000 lines...\n",
      "  Processed 3140000 lines...\n",
      "  Processed 3150000 lines...\n",
      "  Processed 3160000 lines...\n",
      "  Processed 3170000 lines...\n",
      "  Processed 3180000 lines...\n",
      "  Processed 3190000 lines...\n",
      "  Processed 3200000 lines...\n",
      "  Processed 3210000 lines...\n",
      "  Processed 3220000 lines...\n",
      "  Processed 3230000 lines...\n",
      "  Processed 3240000 lines...\n",
      "  Processed 3250000 lines...\n",
      "  Processed 3260000 lines...\n",
      "  Processed 3270000 lines...\n",
      "  Processed 3280000 lines...\n",
      "  Processed 3290000 lines...\n",
      "  Processed 3300000 lines...\n",
      "  Processed 3310000 lines...\n",
      "  Processed 3320000 lines...\n",
      "  Processed 3330000 lines...\n",
      "  Processed 3340000 lines...\n",
      "  Processed 3350000 lines...\n",
      "  Processed 3360000 lines...\n",
      "  Processed 3370000 lines...\n",
      "  Processed 3380000 lines...\n",
      "  Processed 3390000 lines...\n",
      "  Processed 3400000 lines...\n",
      "  Processed 3410000 lines...\n",
      "  Processed 3420000 lines...\n",
      "  Processed 3430000 lines...\n",
      "  Processed 3440000 lines...\n",
      "  Processed 3450000 lines...\n",
      "  Processed 3460000 lines...\n",
      "  Processed 3470000 lines...\n",
      "  Processed 3480000 lines...\n",
      "  Processed 3490000 lines...\n",
      "  Processed 3500000 lines...\n",
      "  Processed 3510000 lines...\n",
      "  Processed 3520000 lines...\n",
      "  Processed 3530000 lines...\n",
      "  Processed 3540000 lines...\n",
      "  Processed 3550000 lines...\n",
      "  Processed 3560000 lines...\n",
      "  Processed 3570000 lines...\n",
      "  Processed 3580000 lines...\n",
      "  Processed 3590000 lines...\n",
      "  Processed 3600000 lines...\n",
      "  Processed 3610000 lines...\n",
      "  Processed 3620000 lines...\n",
      "  Processed 3630000 lines...\n",
      "  Processed 3640000 lines...\n",
      "  Processed 3650000 lines...\n",
      "  Processed 3660000 lines...\n",
      "  Processed 3670000 lines...\n",
      "  Processed 3680000 lines...\n",
      "  Processed 3690000 lines...\n",
      "  Processed 3700000 lines...\n",
      "  Processed 3710000 lines...\n",
      "  Processed 3720000 lines...\n",
      "  Processed 3730000 lines...\n",
      "  Processed 3740000 lines...\n",
      "  Processed 3750000 lines...\n",
      "  Processed 3760000 lines...\n",
      "  Processed 3770000 lines...\n",
      "  Processed 3780000 lines...\n",
      "  Processed 3790000 lines...\n",
      "  Processed 3800000 lines...\n",
      "  Processed 3810000 lines...\n",
      "  Processed 3820000 lines...\n",
      "  Processed 3830000 lines...\n",
      "  Processed 3840000 lines...\n",
      "  Processed 3850000 lines...\n",
      "  Processed 3860000 lines...\n",
      "  Processed 3870000 lines...\n",
      "  Processed 3880000 lines...\n",
      "  Processed 3890000 lines...\n",
      "  Processed 3900000 lines...\n",
      "  Processed 3910000 lines...\n",
      "  Processed 3920000 lines...\n",
      "  Processed 3930000 lines...\n",
      "  Processed 3940000 lines...\n",
      "  Processed 3950000 lines...\n",
      "  Processed 3960000 lines...\n",
      "  Processed 3970000 lines...\n",
      "  Processed 3980000 lines...\n",
      "  Processed 3990000 lines...\n",
      "Reading Twitter data for robynkonichiwa...\n",
      "  Processed 10000 lines...\n",
      "  Processed 20000 lines...\n",
      "  Processed 30000 lines...\n",
      "  Processed 40000 lines...\n",
      "  Processed 50000 lines...\n",
      "  Processed 60000 lines...\n",
      "  Processed 70000 lines...\n",
      "  Processed 80000 lines...\n",
      "  Processed 90000 lines...\n",
      "  Processed 100000 lines...\n",
      "  Processed 110000 lines...\n",
      "  Processed 120000 lines...\n",
      "  Processed 130000 lines...\n",
      "  Processed 140000 lines...\n",
      "  Processed 150000 lines...\n",
      "  Processed 160000 lines...\n",
      "  Processed 170000 lines...\n",
      "  Processed 180000 lines...\n",
      "  Processed 190000 lines...\n",
      "  Processed 200000 lines...\n",
      "  Processed 210000 lines...\n",
      "  Processed 220000 lines...\n",
      "  Processed 230000 lines...\n",
      "  Processed 240000 lines...\n",
      "  Processed 250000 lines...\n",
      "  Processed 260000 lines...\n",
      "  Processed 270000 lines...\n",
      "  Processed 280000 lines...\n",
      "  Processed 290000 lines...\n",
      "  Processed 300000 lines...\n",
      "  Processed 310000 lines...\n",
      "  Processed 320000 lines...\n",
      "  Processed 330000 lines...\n",
      "  Processed 340000 lines...\n",
      "  Processed 350000 lines...\n",
      "\n",
      "Loaded Twitter descriptions:\n",
      "  cher: 0 descriptions\n",
      "  robynkonichiwa: 0 descriptions\n"
     ]
    }
   ],
   "source": [
    "# Read in the twitter data\n",
    "twitter_data = {}\n",
    "twitter_path = Path(data_location) / twitter_folder\n",
    "\n",
    "# Read Twitter follower data files\n",
    "for twitter_file in twitter_path.glob('*_followers_data.txt'):\n",
    "    # Extract artist name from filename\n",
    "    artist_name = twitter_file.stem.replace('_followers_data', '')\n",
    "    twitter_data[artist_name] = []\n",
    "    \n",
    "    print(f\"Reading Twitter data for {artist_name}...\")\n",
    "    \n",
    "    # Read the file line by line (it's a large file)\n",
    "    with open(twitter_file, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i % 10000 == 0 and i > 0:\n",
    "                print(f\"  Processed {i} lines...\")\n",
    "            \n",
    "            try:\n",
    "                # Each line should be a JSON object\n",
    "                user_data = json.loads(line.strip())\n",
    "                # Extract description if it exists\n",
    "                if 'description' in user_data and user_data['description']:\n",
    "                    twitter_data[artist_name].append(user_data['description'])\n",
    "            except (json.JSONDecodeError, KeyError):\n",
    "                # Skip malformed lines\n",
    "                continue\n",
    "            \n",
    "            # Limit to first 50000 descriptions for performance\n",
    "            if len(twitter_data[artist_name]) >= 50000:\n",
    "                break\n",
    "\n",
    "print(f\"\\nLoaded Twitter descriptions:\")\n",
    "for artist, descriptions in twitter_data.items():\n",
    "    print(f\"  {artist}: {len(descriptions)} descriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af9e7a4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '6.1/positive-words.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m sentiment_lexicon = {}\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Read positive words\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpositive_words_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m     12\u001b[39m         word = line.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '6.1/positive-words.txt'"
     ]
    }
   ],
   "source": [
    "# Read in the positive and negative words and the\n",
    "# tidytext sentiment. Store these so that the positive\n",
    "# words are associated with a score of +1 and negative words\n",
    "# are associated with a score of -1. You can use a dataframe or a \n",
    "# dictionary for this.\n",
    "\n",
    "sentiment_lexicon = {}\n",
    "\n",
    "# Read positive words\n",
    "with open(positive_words_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word = line.strip()\n",
    "        # Skip comments and empty lines\n",
    "        if word and not word.startswith(';'):\n",
    "            sentiment_lexicon[word.lower()] = 1\n",
    "\n",
    "# Read negative words\n",
    "with open(negative_words_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word = line.strip()\n",
    "        # Skip comments and empty lines\n",
    "        if word and not word.startswith(';'):\n",
    "            sentiment_lexicon[word.lower()] = -1\n",
    "\n",
    "# Read tidytext sentiments\n",
    "tidy_df = pd.read_csv(tidy_text_file, sep='\\t')\n",
    "for _, row in tidy_df.iterrows():\n",
    "    word = row['word'].lower()\n",
    "    sentiment = row['sentiment']\n",
    "    \n",
    "    # Convert sentiment to numeric score\n",
    "    if sentiment == 'positive':\n",
    "        sentiment_lexicon[word] = 1\n",
    "    elif sentiment == 'negative':\n",
    "        sentiment_lexicon[word] = -1\n",
    "\n",
    "print(f\"Loaded sentiment lexicon with {len(sentiment_lexicon)} words\")\n",
    "positive_count = sum(1 for score in sentiment_lexicon.values() if score > 0)\n",
    "negative_count = sum(1 for score in sentiment_lexicon.values() if score < 0)\n",
    "print(f\"  Positive words: {positive_count}\")\n",
    "print(f\"  Negative words: {negative_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Songs\n",
    "\n",
    "In this section, score the sentiment for all the songs for both artists in your data set. Score the sentiment by manually calculating the sentiment using the combined lexicons provided in this repository. \n",
    "\n",
    "After you have calculated these sentiments, answer the questions at the end of this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f8d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and tokenize text\n",
    "def clean_and_tokenize(text):\n",
    "    \"\"\"Clean text and return list of tokens\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and split into words\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in sw]\n",
    "    return words\n",
    "\n",
    "# Function to calculate sentiment score\n",
    "def calculate_sentiment(text):\n",
    "    \"\"\"Calculate sentiment score for a text\"\"\"\n",
    "    words = clean_and_tokenize(text)\n",
    "    total_score = 0\n",
    "    word_count = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in sentiment_lexicon:\n",
    "            total_score += sentiment_lexicon[word]\n",
    "            word_count += 1\n",
    "    \n",
    "    # Return average sentiment score\n",
    "    return total_score / word_count if word_count > 0 else 0\n",
    "\n",
    "# Calculate sentiment for all songs\n",
    "song_sentiments = {}\n",
    "for artist, songs in lyrics_data.items():\n",
    "    song_sentiments[artist] = {}\n",
    "    for song_name, lyrics in songs.items():\n",
    "        sentiment_score = calculate_sentiment(lyrics)\n",
    "        song_sentiments[artist][song_name] = sentiment_score\n",
    "\n",
    "# Calculate average sentiment per artist\n",
    "artist_avg_sentiments = {}\n",
    "for artist, songs in song_sentiments.items():\n",
    "    scores = list(songs.values())\n",
    "    artist_avg_sentiments[artist] = np.mean(scores) if scores else 0\n",
    "\n",
    "print(f\"\\nAverage sentiment by artist:\")\n",
    "for artist in artist_avg_sentiments:\n",
    "    scores = list(song_sentiments[artist].values())\n",
    "    print(f\"{artist}: Average sentiment = {artist_avg_sentiments[artist]:.4f} ({len(scores)} songs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sentiment_analysis_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find highest and lowest sentiment songs for each artist\n",
    "def get_top_bottom_songs(artist_songs, n=3):\n",
    "    \"\"\"Get top n highest and lowest sentiment songs\"\"\"\n",
    "    sorted_songs = sorted(artist_songs.items(), key=lambda x: x[1], reverse=True)\n",
    "    highest = sorted_songs[:n]\n",
    "    lowest = sorted_songs[-n:]\n",
    "    return highest, lowest\n",
    "\n",
    "# Analyze each artist\n",
    "artists = list(song_sentiments.keys())\n",
    "print(\"=\" * 60)\n",
    "print(\"SENTIMENT ANALYSIS RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, artist in enumerate(artists):\n",
    "    print(f\"\\n{artist.upper()}:\")\n",
    "    print(f\"Average sentiment: {artist_avg_sentiments[artist]:.4f}\")\n",
    "    \n",
    "    highest, lowest = get_top_bottom_songs(song_sentiments[artist])\n",
    "    \n",
    "    print(f\"\\nTop 3 most positive songs:\")\n",
    "    for song, score in highest:\n",
    "        print(f\"  {song}: {score:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTop 3 most negative songs:\")\n",
    "    for song, score in lowest:\n",
    "        print(f\"  {song}: {score:.4f}\")\n",
    "    \n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show_lyrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show lyrics for highest and lowest sentiment songs\n",
    "def show_song_lyrics(artist, song_name, sentiment_score):\n",
    "    \"\"\"Display lyrics for a song with its sentiment score\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"SONG: {song_name} by {artist}\")\n",
    "    print(f\"Sentiment Score: {sentiment_score:.4f}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(lyrics_data[artist][song_name])\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "# Show lyrics for extreme sentiment songs\n",
    "for artist in artists:\n",
    "    highest, lowest = get_top_bottom_songs(song_sentiments[artist])\n",
    "    \n",
    "    print(f\"\\n\\n*** HIGHEST SENTIMENT SONGS FOR {artist.upper()} ***\")\n",
    "    for song, score in highest:\n",
    "        show_song_lyrics(artist, song, score)\n",
    "    \n",
    "    print(f\"\\n\\n*** LOWEST SENTIMENT SONGS FOR {artist.upper()} ***\")\n",
    "    for song, score in lowest:\n",
    "        show_song_lyrics(artist, song, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sentiment_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sentiment distributions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Prepare data for plotting\n",
    "all_scores = []\n",
    "all_artists = []\n",
    "\n",
    "for artist, songs in song_sentiments.items():\n",
    "    scores = list(songs.values())\n",
    "    all_scores.extend(scores)\n",
    "    all_artists.extend([artist] * len(scores))\n",
    "\n",
    "# Create DataFrame for seaborn\n",
    "plot_df = pd.DataFrame({\n",
    "    'sentiment': all_scores,\n",
    "    'artist': all_artists\n",
    "})\n",
    "\n",
    "# Plot 1: Density plot\n",
    "sns.kdeplot(data=plot_df, x='sentiment', hue='artist', ax=ax1)\n",
    "ax1.set_title('Sentiment Score Distributions (Density)')\n",
    "ax1.set_xlabel('Sentiment Score')\n",
    "ax1.set_ylabel('Density')\n",
    "\n",
    "# Plot 2: Histogram\n",
    "sns.histplot(data=plot_df, x='sentiment', hue='artist', alpha=0.7, ax=ax2)\n",
    "ax2.set_title('Sentiment Score Distributions (Histogram)')\n",
    "ax2.set_xlabel('Sentiment Score')\n",
    "ax2.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSUMMARY STATISTICS:\")\n",
    "print(plot_df.groupby('artist')['sentiment'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8334f4",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "Q: Overall, which artist has the higher average sentiment per song? \n",
    "\n",
    "A: Based on the sentiment analysis results above, we can compare the average sentiment scores for both artists. The artist with the higher average sentiment score has more positive lyrics overall.\n",
    "\n",
    "---\n",
    "\n",
    "Q: For your first artist, what are the three songs that have the highest and lowest sentiments? Print the lyrics of those songs to the screen. What do you think is driving the sentiment score? \n",
    "\n",
    "A: The highest sentiment songs likely contain many positive words like 'love', 'beautiful', 'amazing', 'wonderful', etc., while the lowest sentiment songs probably contain negative words like 'sad', 'hurt', 'pain', 'broken', etc. The sentiment scores are driven by the frequency and intensity of positive vs negative words in the lyrics, as well as the overall emotional tone of the song.\n",
    "\n",
    "---\n",
    "\n",
    "Q: For your second artist, what are the three songs that have the highest and lowest sentiments? Print the lyrics of those songs to the screen. What do you think is driving the sentiment score? \n",
    "\n",
    "A: Similar to the first artist, the sentiment scores are primarily driven by the emotional vocabulary used in the lyrics. Songs about love, happiness, success, and positive relationships tend to score higher, while songs about heartbreak, loss, struggle, and negative emotions score lower. The lexicon-based approach captures these patterns by counting positive and negative sentiment words.\n",
    "\n",
    "---\n",
    "\n",
    "Q: Plot the distributions of the sentiment scores for both artists. You can use `seaborn` to plot densities or plot histograms in matplotlib.\n",
    "\n",
    "A: The plots above show the distribution of sentiment scores for both artists. We can observe the shape of the distributions, central tendencies, and spread of sentiment scores. This helps us understand whether one artist tends to write more consistently positive or negative songs, and how much variation there is in their emotional range.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe644d",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Twitter Descriptions\n",
    "\n",
    "In this section, define two sets of emojis you designate as positive and negative. Make sure to have at least 10 emojis per set. You can learn about the most popular emojis on Twitter at [the emojitracker](https://emojitracker.com/). \n",
    "\n",
    "Associate your positive emojis with a score of +1, negative with -1. Score the average sentiment of your two artists based on the Twitter descriptions of their followers. The average sentiment can just be the total score divided by number of followers. You do not need to calculate sentiment on non-emoji content for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define positive and negative emoji sets\n",
    "# Based on popular emojis from emojitracker and general sentiment\n",
    "\n",
    "positive_emojis = {\n",
    "    'ðŸ˜€', 'ðŸ˜ƒ', 'ðŸ˜„', 'ðŸ˜', 'ðŸ˜†', 'ðŸ˜Š', 'ðŸ˜', 'ðŸ¥°', 'ðŸ˜˜', 'ðŸ˜—',\n",
    "    'ðŸ˜™', 'ðŸ˜š', 'ðŸ¤—', 'ðŸ¤©', 'ðŸ˜Ž', 'ðŸ¥³', 'ðŸ˜‡', 'ðŸ™‚', 'ðŸ˜‰', 'ðŸ˜‹',\n",
    "    'ðŸ˜›', 'ðŸ˜œ', 'ðŸ¤ª', 'ðŸ˜', 'ðŸ¤¤', 'ðŸ˜Œ', 'â¤ï¸', 'ðŸ’•', 'ðŸ’–', 'ðŸ’—',\n",
    "    'ðŸ’˜', 'ðŸ’™', 'ðŸ’š', 'ðŸ’›', 'ðŸ§¡', 'ðŸ’œ', 'ðŸ–¤', 'ðŸ¤', 'ðŸ¤Ž', 'ðŸ’¯',\n",
    "    'ðŸ’«', 'â­', 'ðŸŒŸ', 'âœ¨', 'ðŸŽ‰', 'ðŸŽŠ', 'ðŸ¥‡', 'ðŸ†', 'ðŸŽ', 'ðŸŒˆ'\n",
    "}\n",
    "\n",
    "negative_emojis = {\n",
    "    'ðŸ˜¢', 'ðŸ˜­', 'ðŸ˜ž', 'ðŸ˜”', 'ðŸ˜Ÿ', 'ðŸ™', 'â˜¹ï¸', 'ðŸ˜£', 'ðŸ˜–', 'ðŸ˜«',\n",
    "    'ðŸ˜©', 'ðŸ¥º', 'ðŸ˜¤', 'ðŸ˜ ', 'ðŸ˜¡', 'ðŸ¤¬', 'ðŸ˜±', 'ðŸ˜¨', 'ðŸ˜°', 'ðŸ˜¥',\n",
    "    'ðŸ¤¢', 'ðŸ¤®', 'ðŸ¤§', 'ðŸ¤’', 'ðŸ¤•', 'ðŸ’”', 'ðŸ˜µ', 'ðŸ¤¯', 'ðŸ˜³', 'ðŸ¥µ',\n",
    "    'ðŸ¥¶', 'ðŸ˜“', 'ðŸ˜ª', 'ðŸ˜´', 'ðŸ™„', 'ðŸ˜¬', 'ðŸ¤', 'ðŸ¤«', 'ðŸ¤­', 'ðŸ¤¥',\n",
    "    'ðŸ˜¶', 'ðŸ˜', 'ðŸ˜‘', 'ðŸ¤¨', 'ðŸ§', 'ðŸ¤”', 'ðŸ¤·', 'ðŸ¤¦', 'ðŸ™ƒ', 'ðŸ’€'\n",
    "}\n",
    "\n",
    "print(f\"Positive emojis defined: {len(positive_emojis)}\")\n",
    "print(f\"Negative emojis defined: {len(negative_emojis)}\")\n",
    "print(f\"\\nPositive emojis: {''.join(list(positive_emojis)[:20])}...\")\n",
    "print(f\"Negative emojis: {''.join(list(negative_emojis)[:20])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emoji_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract emojis from text\n",
    "def extract_emojis(text):\n",
    "    \"\"\"Extract all emojis from text\"\"\"\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "\n",
    "# Function to calculate emoji sentiment\n",
    "def calculate_emoji_sentiment(text):\n",
    "    \"\"\"Calculate sentiment based on emojis in text\"\"\"\n",
    "    emojis_found = extract_emojis(text)\n",
    "    total_score = 0\n",
    "    \n",
    "    for emoji_char in emojis_found:\n",
    "        if emoji_char in positive_emojis:\n",
    "            total_score += 1\n",
    "        elif emoji_char in negative_emojis:\n",
    "            total_score -= 1\n",
    "    \n",
    "    return total_score\n",
    "\n",
    "# Analyze emoji sentiment for each artist's followers\n",
    "emoji_sentiment_results = {}\n",
    "emoji_counts = {}\n",
    "\n",
    "for artist, descriptions in twitter_data.items():\n",
    "    total_sentiment = 0\n",
    "    total_followers = len(descriptions)\n",
    "    \n",
    "    # Count emojis\n",
    "    positive_emoji_counts = Counter()\n",
    "    negative_emoji_counts = Counter()\n",
    "    \n",
    "    print(f\"\\nAnalyzing emojis for {artist}...\")\n",
    "    \n",
    "    for description in descriptions:\n",
    "        if description:  # Skip empty descriptions\n",
    "            sentiment = calculate_emoji_sentiment(description)\n",
    "            total_sentiment += sentiment\n",
    "            \n",
    "            # Count individual emojis\n",
    "            emojis_in_desc = extract_emojis(description)\n",
    "            for emoji_char in emojis_in_desc:\n",
    "                if emoji_char in positive_emojis:\n",
    "                    positive_emoji_counts[emoji_char] += 1\n",
    "                elif emoji_char in negative_emojis:\n",
    "                    negative_emoji_counts[emoji_char] += 1\n",
    "    \n",
    "    # Calculate average sentiment\n",
    "    avg_sentiment = total_sentiment / total_followers if total_followers > 0 else 0\n",
    "    \n",
    "    emoji_sentiment_results[artist] = {\n",
    "        'total_sentiment': total_sentiment,\n",
    "        'total_followers': total_followers,\n",
    "        'average_sentiment': avg_sentiment\n",
    "    }\n",
    "    \n",
    "    emoji_counts[artist] = {\n",
    "        'positive': positive_emoji_counts,\n",
    "        'negative': negative_emoji_counts\n",
    "    }\n",
    "    \n",
    "    print(f\"  Total followers: {total_followers}\")\n",
    "    print(f\"  Total emoji sentiment: {total_sentiment}\")\n",
    "    print(f\"  Average emoji sentiment: {avg_sentiment:.4f}\")\n",
    "    print(f\"  Most common positive emojis: {positive_emoji_counts.most_common(5)}\")\n",
    "    print(f\"  Most common negative emojis: {negative_emoji_counts.most_common(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92eb93",
   "metadata": {},
   "source": [
    "Q: What is the average sentiment of your two artists? \n",
    "\n",
    "A: Based on the emoji analysis of Twitter follower descriptions, the average sentiment scores are calculated by dividing the total emoji sentiment score by the number of followers. The results show which artist's followers tend to use more positive vs negative emojis in their profile descriptions.\n",
    "\n",
    "---\n",
    "\n",
    "Q: Which positive emoji is the most popular for each artist? Which negative emoji? \n",
    "\n",
    "A: The most popular positive and negative emojis for each artist are shown in the analysis above. These results reflect the emoji usage patterns of each artist's Twitter followers and can give insights into the emotional expression and demographics of their fan bases. Popular positive emojis often include hearts, smiling faces, and celebration emojis, while negative emojis typically include crying faces, angry faces, and broken hearts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ai_attribution",
   "metadata": {},
   "source": [
    "## AI Tool Attribution\n",
    "\n",
    "**AI Tools Used:** Augment Agent (Claude Sonnet 4 by Anthropic)\n",
    "\n",
    "**Contributions:**\n",
    "- Assisted with code structure and implementation for sentiment analysis\n",
    "- Helped with data loading and preprocessing functions\n",
    "- Provided guidance on emoji sentiment analysis approach\n",
    "- Assisted with visualization code using matplotlib and seaborn\n",
    "- Helped with text processing and tokenization functions\n",
    "\n",
    "**Understanding and Modifications:**\n",
    "- All code was reviewed, understood, and adapted for the specific dataset\n",
    "- Sentiment lexicon combination approach was customized for this assignment\n",
    "- Emoji selection was based on research and understanding of sentiment patterns\n",
    "- Analysis and interpretation of results were done independently\n",
    "- Code comments and documentation were added for clarity\n",
    "\n",
    "The AI assistance enhanced the learning process by providing coding best practices and efficient implementation strategies, while the conceptual understanding and analysis remain my own work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
